{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd879793",
   "metadata": {},
   "source": [
    "## Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "from recsysNN_utils import *\n",
    "from public_tests_cbf import *\n",
    "\n",
    "pd.set_option(\"display.precision\", 1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre=load_data()\n",
    "\n",
    "num_user_features=user_train.shape[1]-3\n",
    "num_item_features=item_train.shape[1]-1\n",
    "\n",
    "uvs=3\n",
    "ivs=3\n",
    "u_s=3\n",
    "i_s=1\n",
    "scale_data=True\n",
    "\n",
    "print(f\"Number of training vectors: {len(item_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training data\n",
    "\n",
    "if scale_data:\n",
    "  item_train_save=item_train\n",
    "  user_train_save=user_train\n",
    "\n",
    "  # Create the scaler object\n",
    "  scaler_item=StandardScaler()\n",
    "  # Compute mean and std\n",
    "  scaler_item.fit(item_train)\n",
    "  # Scale using learned mean/std\n",
    "  item_train=scaler_item.transform(item_train)\n",
    "\n",
    "  scaler_user=StandardScaler()\n",
    "  scaler_user.fit(user_train)\n",
    "  user_train=scaler_user.transform(user_train)\n",
    "\n",
    "  print(np.allclose(item_train_save, scaler_item.inverse_transform(item_train)))\n",
    "  print(np.allclose(user_train_save, scaler_user.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train, item_test=train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "user_train, user_test=train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test=train_test_split(y_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "\n",
    "print(f\"Item training data: {item_train.shape}\")\n",
    "print(f\"Item testing data: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the target between -1 and 1\n",
    "\n",
    "scaler=MinMaxScaler((-1, 1))\n",
    "scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "y_norm_train=scaler.transform(y_train.reshape(-1, 1))\n",
    "y_norm_test=scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(y_norm_train.shape, y_norm_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677cdd7",
   "metadata": {},
   "source": [
    "### Neural Network for Content Based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs=32\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "user_NN=tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_outputs, activation='linear')\n",
    "])\n",
    "\n",
    "item_NN=tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_outputs, activation='linear')\n",
    "])\n",
    "\n",
    "# Create input and point to the base network\n",
    "input_user=tf.keras.layers.Input(shape=(num_user_features,))\n",
    "vu=user_NN(input_user)\n",
    "vu=tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vu)\n",
    "\n",
    "input_item=tf.keras.layers.Input(shape=(num_item_features,))\n",
    "vm=item_NN(input_item)\n",
    "vm=tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vm)\n",
    "\n",
    "# Compute dot product of two vectors vu and vm\n",
    "output=tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# Specify inputs and output to the model\n",
    "model=Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "cost_fn=tf.keras.losses.MeanSquaredError()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_norm_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6890d1",
   "metadata": {},
   "source": [
    "### Predictions for a New User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_id=5000\n",
    "new_rating_ave=1.0\n",
    "new_action=1.0\n",
    "new_adventure=1\n",
    "new_animation=1\n",
    "new_childrens=1\n",
    "new_comedy=5\n",
    "new_crime=1\n",
    "new_documentary=1\n",
    "new_drama=1\n",
    "new_fantasy=1\n",
    "new_horror=1\n",
    "new_mystery=1\n",
    "new_romance=5\n",
    "new_scifi=5\n",
    "new_thriller=1\n",
    "new_rating_count=3\n",
    "\n",
    "user_vec=np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "                    new_action, new_adventure, new_animation, new_childrens,\n",
    "                    new_comedy, new_crime, new_documentary,\n",
    "                    new_drama, new_fantasy, new_horror, new_mystery,\n",
    "                    new_romance, new_scifi, new_thriller]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and replicate the user vector to match the number of movies in the dataset\n",
    "user_vecs=gen_user_vecs(user_vec, len(item_vecs))\n",
    "\n",
    "# Scale the vector and make predictions for all movies\n",
    "sorted_idx, sorted_ypu, sorted_items, sorted_user=predict_uservec(user_vecs, item_vecs, model, u_s, i_s,\n",
    "                                                                  scaler, scaler_user, scaler_item, scaledata=scale_data)\n",
    "\n",
    "print_pred_movies(sorted_ypu, sorted_user, sorted_items, movie_dict, maxcount=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d4d76",
   "metadata": {},
   "source": [
    "### Predictions for an Existing User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6465ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=36\n",
    "\n",
    "user_vecs, y_vecs=get_user_vecs(uid, scaler_user.inverse_transform(user_train), item_vecs, user_to_genre)\n",
    "\n",
    "sorted_idx, sorted_ypu, sorted_items, sorted_user=predict_uservec(user_vecs, item_vecs, model, u_s, i_s,\n",
    "                                                                  scaler, scaler_user, scaler_item, scaledata=scale_data)\n",
    "\n",
    "sorted_y=y_vecs[sorted_idx]\n",
    "\n",
    "print_existing_user(sorted_ypu, sorted_y.reshape(-1, 1), sorted_user, sorted_items, item_features, ivs, uvs, movie_dict, maxcount=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8409d6d",
   "metadata": {},
   "source": [
    "### Finding Similar Items\n",
    "\n",
    "A similarity measure is the squared distance between the two vectors $ \\mathbf{v_m^{(k)}}$ and $\\mathbf{v_m^{(i)}}$ :\n",
    "$$\\left\\Vert \\mathbf{v_m^{(k)}} - \\mathbf{v_m^{(i)}}  \\right\\Vert^2 = \\sum_{l=1}^{n}(v_{m_l}^{(k)} - v_{m_l}^{(i)})^2\\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ec190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_dist(a, b):\n",
    "  d=sum(np.square(a-b))\n",
    "  return (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sq_dist(sq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc546fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1.1, 2.1, 3.1])\n",
    "b=np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "dist=sq_dist(a, b)\n",
    "print(f\"Squared distance between a and b: {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_item_m=tf.keras.layers.Input(shape=(num_item_features,))\n",
    "vm_m=item_NN(input_item_m)\n",
    "vm_m=tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vm_m)\n",
    "model_m=Model(input_item_m, vm_m)\n",
    "model_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30761",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_item_vecs=scaler_item.transform(item_vecs)\n",
    "vms=model_m.predict(scaled_item_vecs[:, i_s:])\n",
    "print(f\"Size of all predicted movie feature vectors: {vm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=50\n",
    "dim=len(vms)\n",
    "dist=np.zeros((dim, dim))\n",
    "\n",
    "for i in range(dim):\n",
    "  for j in range(dim):\n",
    "    dist[i, j]=sq_dist(vms[i, :], vms[j, :])\n",
    "\n",
    "m_dist=ma.masked_array(dist, mask=np.identity(dist.shape[0]))\n",
    "\n",
    "disp=[['movie1', 'genres', 'movie2', 'genres']]\n",
    "for i in range(count):\n",
    "  min_idx=np.argmin(m_dist[i])\n",
    "  movie1_id=int(item_vecs[i, 0])\n",
    "  movie2_id=int(item_vecs[min_idx, 0])\n",
    "  genre1, _=get_item_genre(item_vecs[i, :], ivs, item_features)\n",
    "  genre2, _=get_item_genre(item_vecs[min_idx, :], ivs, item_features)\n",
    "\n",
    "  disp.append([\n",
    "    movie_dict[movie1_id]['title'], genre1,\n",
    "    movie_dict[movie2_id]['title'], genre2\n",
    "  ])\n",
    "\n",
    "table=tabulate.tabulate(disp, tablefmt='html', headers='firstrow', floatfmt=[\".1f\", \".1f\", \".0f\", \".2f\", \".2f\"])\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
